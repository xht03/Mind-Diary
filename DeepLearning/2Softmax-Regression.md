---
title: 单层神经网络2：softmax回归
date: 2025-03-16 16:56:10
tags:
- labs
- notes
categories:
- Deep Learning
---

### softmax回归

线性回归模型适⽤于输出为连续值的情景。但在分类问题中，比如图像识别，我们希望模型能告诉我们图像属于哪一个类别，所以输出为离散值。如何将线性回归模型改造成适用于分类问题的模型呢？如何将连续值的输出转换为离散值的输出呢？

一个朴素的想法就是：如果输出层的神经元个数等于类别数，那么我们可以将输出层的神经元的输出值看作是属于各个类别的概率，即概率分布。概率最高的类别即为模型的预测输出。这样，我们就可以使用线性回归模型来解决分类问题了。这种模型叫做**softmax回归模型**。


---
### softmax模型

让我们考虑一个简单的图像分类问题，其中输入图像的高和宽均为 2 像素，且色彩为灰度。这样，每个像素值都可以用一个标量表示。我们将图像中的 4 个像素分别记为 $x_1, x_2, x_3, x_4$。

假设训练数据集中的图像的真实标签为狗、猫或鸡，并且我们将这 3 种动物的标签分别对应为离散值 $y_1, y_2, y_3$。

我们可以写该模型的预测表达式：

$$
\begin{bmatrix}
o_1 \\
o_2 \\
o_3
\end{bmatrix}
=
\begin{bmatrix}
w_{11} & w_{12} & w_{13} & w_{14} \\
w_{21} & w_{22} & w_{23} & w_{24} \\
w_{31} & w_{32} & w_{33} & w_{34}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
b_3
\end{bmatrix}
$$

即：
$$
\mathbf{o} = \mathbf{w} \mathbf{x} + \mathbf{b} \\
\hat{\mathbf{y}} = \text{softmax}(\mathbf{o})
$$

![](https://ref.xht03.online/202503161708931.png)

---

### softmax运算

在分类问题中，我们需要得到离散的预测输出。一个简单的方法是将输出值 $o_i$ 视为预测类别 $i$ 的**置信度**，并将取值最大的输出所对应的类别作为最终预测结果，即：$ \hat{y} = \arg\max_i o_i $。例如，若模型输出：$ o_1 = 0.1, \ o_2 = 10, \ o_3 = 0.1 $，由于 $o_2$ 最大，因此预测类别为 $2$，对应的类别为 "猫"。

但这么做有两个问题：

- 由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例⼦中的输出值10表⽰“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍，但如果 $o_1 = o_3 = 1000$ 呢？这时我们就很难判断这个输出值的意义了。

- 由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量（如何将模型的输出与离散的真实标签进行有效比较）。

为了解决这两个问题，我们引入**softmax运算**：

$$
\hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}
$$

不难看出 $\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$，因此 $\hat{y}_i$ 可以看作是预测类别为 $i$ 的概率。且 softmax 运算不改变相对大小，因此不会改变预测类别。

---

### softmax误差衡量

前面简略地提到过，使用 softmax 运算后，可以更方便地计算预测结果与离散标签之间的误差。这里具体解释一下。

我们已经知道，softmax 运算可以将模型的输出转换为一个合法的类别预测分布。实际上，真实标签也可以用 **类别分布** 来表示：对于样本 $i$，我们构造一个向量 $y^{(i)} \in \mathbb{R}^q$，其中第 $y^{(i)}$（即样本 $i$ 的真实类别）个元素设为 $1$，其余元素设为 $0$。  

这样，我们的训练目标就是使模型的预测概率分布 $\hat{y}^{(i)}$ **尽可能接近** 真实的标签概率分布 $y^{(i)}$。

我们仍可以像线性回归那样使用 **平方损失函数** 来衡量误差，其形式为：  

$$
\frac{1}{2} \|\hat{y}^{(i)} - y^{(i)}\|^2
$$

然而，在分类问题中，我们的目标是 **预测的类别正确**，而不需要预测的概率完全等于标签概率。例如，在图像分类任务中，如果样本 $i$ 的真实类别为 $y^{(i)} = 3$，那么我们只需要 **$\hat{y}^{(i)}_3$ 比其他两个预测值 $\hat{y}^{(i)}_1$ 和 $\hat{y}^{(i)}_2$ 更大**，就已经满足分类正确的要求。  

例如 $\hat{y}^{(i)}_3 = 0.6$，不论 $\hat{y}^{(i)}_1$ 和 $\hat{y}^{(i)}_2$ 取何值，只要 $\hat{y}^{(i)}_3$ 最大，分类结果仍然是正确的。  

然而，平方损失函数往往 **过于严格**，例如以下两种预测情况：

- **情况 1**：$\hat{y}^{(i)}_1 = \hat{y}^{(i)}_2 = 0.2$，$\hat{y}^{(i)}_3 = 0.6$

- **情况 2**：$\hat{y}^{(i)}_1 = 0, \hat{y}^{(i)}_2 = 0.4, \hat{y}^{(i)}_3 = 0.6$

尽管两者的分类结果都是正确的（因为 $\hat{y}^{(i)}_3$ 依然最大），但 **平方损失函数在情况 1 中的损失要小很多**，这可能导致模型优化方向的偏差。因此，**在分类任务中，平方损失函数往往并不是最优选择**。

---

### 交叉熵

交叉熵用于衡量两个概率分布之间的差异。理解交叉熵可以从直觉和数学两个层面入手。

#### 直觉理解

想象你有一个真实的事件分布，比如掷骰子的结果，假设是个均匀分布（每个面概率都是 1/6）。现在你用一个模型去预测这个分布，但模型给出的预测分布可能不完全准确，比如它认为某些面出现的概率更高。交叉熵就是用来衡量你的模型预测分布和真实分布之间的“差距”有多大。差距越大，交叉熵的值就越大。

换句话说，交叉熵有点像是在问：“如果我用这个不太完美的模型去描述真实世界，我需要付出多少额外的‘信息代价’？”（这里的“信息代价”可以用编码的比特数来理解。）

#### 数学定义

对于两个概率分布 $P$ 和 $Q$，交叉熵 $ H(P, Q) $ 定义为：

$$
H(P, Q) = -\sum_{x} P(x) \log(Q(x))
$$

其中：

- $ P(x) $：事件 $x$ 在真实分布中的概率。

- $ Q(x) $：事件 $x$ 在预测分布中的概率。

#### 和香农熵的关系

交叉熵跟香农熵（Shannon Entropy）有密切联系。香农熵 $ H(P) $ 是描述分布 $ P $ 自身不确定性的量：

$$
H(P) = -\sum_{x} P(x) \log(P(x))
$$

而交叉熵 $ H(P, Q) $ 是用 $ Q $ 去描述 $ P $ 时的信息量。两者的差就是著名的 KL 散度（Kullback-Leibler Divergence），它直接衡量了 $ P $ 和 $ Q $ 的差异：

$$
H(P, Q) = H(P) + KL(P || Q)
$$

交叉熵可以被视为香农熵的扩展。由于 KL 散度是非负的，因此交叉熵总是大于等于香农熵。当 $P$ 和 $Q$ 相同（即 $P=Q$）时，交叉熵等于香农熵：

$$
H(P, Q) = H(P)
$$

---

### softmax损失函数

有了上述背景知识，我们知道：在分类问题中，交叉熵比平方损失函数更适合衡量模型输出与真实标签之间的误差。因此，我们可以使用交叉熵损失函数来衡量 softmax 回归的误差。

对于样本 $i$，其交叉熵损失函数定义为：

$$
l^{(i)}(\mathbf{w}, \mathbf{b}) = -\sum_j^q y_j^{(i)} \log(\hat{y}_j^{(i)})
$$

其中：

- $q$：类别数。

- $y_j^{(i)}$：样本 $i$ 的真实标签中第 $j$ 个元素（即样本 $i$ 的真实类别）。

- $\hat{y}_j^{(i)}$：样本 $i$ 的预测标签中第 $j$ 个元素（即样本 $i$ 的预测类别）。

