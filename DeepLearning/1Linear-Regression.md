---
title: 单层神经网络1：线性回归
date: 2025-03-16 15:59:10
tags:
- labs
- notes
categories:
- Deep Learning
---

### 线性回归

深度学习常需要解决的两类问题：**回归问题**和**分类问题**。回归问题在实际中很常⻅，如预测房屋价格、⽓温、销售额等连续值的问题。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题则属于分类问题。

**线性回归**输出是⼀个连续值，因此适⽤于回归问题。**softmax回归**则适⽤于分类问题，它的输出是⼀个概率分布。

---

### 线性回归模型

我们以⼀个简单的房屋价格预测作为例⼦来解释线性回归的基本要素。

设房屋面积为 $x_1$，房龄为 $x_2$，楼层为 $x_3$，售出价格为 $y$，我们希望建立一个基于输入 $x_1$，$x_2$，$x_3$ 来计算输出 $y$ 的数学模型，即：  

$$
\hat{y} = w_1 x_1 + w_2 x_2 + w_3 x_3 + b
$$

其中：  
- $w_1$ 和 $w_2$ 是**权重（weight）**，表示每个特征对价格的影响程度；

- $b$ 是**偏差（bias）**，用于调整整体预测值；

- 这些参数 $\{w_1, w_2, b\}$ 统称为**模型参数（parameter）**；

- $\hat{y}$ 是模型的预测值，而真实值为 $y$。

由于数据可能存在噪声，模型预测 $\hat{y}$ 和真实价格 $y$ 之间可能存在一定误差。

---

### 模型训练

沿用房价预测的例子，现在，我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作**模型训练（model training）**。

1. 训练集
    
    - 我们通常收集⼀系列的真实数据，例如多栋房屋的真实售出价格和它们对应的⾯积、房龄、楼层。我们希望在这个数据上⾯寻找模型参数来使模型的预测价格与真实价格的误差最小。
    
    - 在机器学习术语⾥，该数据集被称为**训练集（training set）**
    
    - ⼀栋房屋被称为⼀个**样本（sample）**
    
    - 其真实售出价格叫作**标签（label）**
    
    - ⽤来预测标签的三个因素叫作**特征（feature）**。特征⽤来表征样本的特点。

2. 损失函数

    - 在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取⼀个⾮负数作为误差，且数值越小表⽰误差越小。⼀个常⽤的选择是平⽅函数。

    - 平方损失函数定义为（其中常数 $\frac{1}{2}$ 使对平⽅项求导后的常数系数为1，这样在形式上稍微简单⼀些）：

    $$
    l(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2
    $$

    - 在训练模型时，我们希望找出一组模型参数，记为 $\{w_1, w_2, b\}$，来使训练样本的损失函数最小。

3. 优化算法

    - 当模型和损失函数形式较为简单时，上⾯的损失函数最小化问题的解可以直接通过求导获得。这类解叫作解析解（analytical solution）。上述房价预测的例子使⽤的线性回归和平⽅误差刚好属于这个范畴。
    
    - 然而，⼤多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作**数值解（numerical solution）**。

    - 在求数值解的优化算法中，**小批量随机梯度下降（mini-batch stochastic gradient descent）**应用广泛。

---

### 小批量随机梯度下降

它的算法很简单：

- 先选取⼀组模型参数的初始值，如随机选取

- 对参数进⾏多次迭代，使每次迭代都可能降低损失函数的值。

- 在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$。然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本次迭代的减小量。

它的迭代公式如下：

$$
w_1 \leftarrow w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{\partial l^{(i)}(w_1, w_2, b)}{\partial w_1} \\

w_2 \leftarrow w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{\partial l^{(i)}(w_1, w_2, b)}{\partial w_2} \\

w_3 \leftarrow w_3 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{\partial l^{(i)}(w_1, w_2, b)}{\partial w_3} \\

b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{\partial l^{(i)}(w_1, w_2, b)}{\partial b}
$$

在上式中，$\mathcal{B}$ 代表每个小批量中的样本个数（批量⼤小，batch size），$\eta$ 称作学习率（learningrate）并取正数。需要强调的是，这⾥的批量⼤小和学习率的值是⼈为设定的，并不是通过模型训练学出的，因此叫作**超参数（hyperparameter）**。我们通常所说的“调参”指的正是调节超参数。

