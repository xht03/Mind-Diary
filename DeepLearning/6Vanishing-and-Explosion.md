---
title: "衰减和爆炸"
author: "xht03"
date: 2025-04-19 9:48:20
tags:
- labs
- notes
categories:
- Deep Learning
header-includes:
- \usepackage{xeCJK}
---


## 数值稳定性

当神经网络的层数较多时，模型的数值稳定性容易变差。考虑一个层数为 $L$ 的多层感知机（MLP），为了简化分析，我们假设：

- 第 $l$ 层 $H^{(l)}$ 的权重参数为 $W^{(l)}$

- 输出层 $H^{(L)}$ 的权重参数为 $W^{(L)}$

- 假设条件：

  - 不考虑偏差参数

  - 所有隐藏层的激活函数为恒等映射 $\phi(x) = x$

给定输入 $ X $，第 $ l $ 层的输出为：
$$ H^{(l)} = X W^{(1)} W^{(2)} \cdots W^{(l)} $$

当层数 $ l $ 较大时，$ H^{(l)} $ 的计算可能出现：

1. ​**衰减（Vanishing）​**：输出值趋近于0

2. ​**爆炸（Exploding）​**：输出值趋近于无穷大

---

例如，简便起见，假设输入和所有权重参数都是标量：

- 当权重 $ W = 0.2 $ 时：
  $$
  H^{(30)} = X \times 0.2^{30} \approx X \times 1 \times 10^{-21} \quad \text{(衰减)}
  $$
  
- 当权重 $ W = 5 $ 时：
  $$
  H^{(30)} = X \times 5^{30} \approx X \times 9 \times 10^{20} \quad \text{(爆炸)}
  $$

衰减和爆炸不止会影响正向传播，还会影响反向传播中的梯度计算。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸现象。
