---
title: "⽋拟合与过拟合"
author: "xht03"
date: 2025-03-21 14:42:10
tags:
- labs
- notes
categories:
- Deep Learning
header-includes:
- \usepackage{xeCJK}
---

## ⽋拟合与过拟合

在机器学习中，⽋拟合（underfitting）和过拟合（overfitting）是模型训练中经常出现的两类典型问题。

- **⽋拟合**：模型⽆法得到较低的训练误差。

- **过拟合**：模型的训练误差远小于它在测试集上的误差。

虽然有很多因素可能导致这两种拟合问题，在这⾥我们重点讨论两个因素：**模型复杂度**和**训练集⼤小**。

### 模型复杂度

我们以多项式函数拟合为例。

给定一个由标量数据特征 $x$ 和对应的标量标签 $y$ 组成的训练集，多项式函数拟合的目标是找到一个 $K$ 阶多项式函数来近似 $y$ ：

$$
\hat{y} = b + \sum_{k=1}^{K} x^k w_k
$$

在上式中，$w_k$ 是模型的权重参数，$b$ 是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又称为线性函数拟合。

因为⾼阶多项式函数模型参数更多，模型函数的选择空间更⼤，所以⾼阶多项式函数⽐低阶多项式函数的复杂度更⾼。因此，⾼阶多项式函数⽐低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。

给定训练集，模型复杂度和误差之间的关系可以用下图来表示：

- 如果模型的复杂度过低，很容易出现⽋拟合。

- 如果模型复杂度过⾼，很容易出现过拟合。

![](https://ref.xht03.online/202503211451127.png)

### 训练集⼤小

⼀般来说，

- 如果训练集中样本数过少，特别是⽐模型参数数量（按元素计）更少时，**过拟合**更容易发⽣。

- 此外，泛化误差不会随训练集⾥样本数量增加而增⼤。

因此，在计算资源允许的范围之内，我们通常希望训练集⼤⼀些。

---

## 权重衰减

权重衰减等价于 L2 范数正则化（regularization），是应对过拟合的一种常用方法。

- L2 范数正则化在模型原损失函数基础上添加 L2 范数惩罚项，从而得到训练所需要最小化的函数。

- L2 范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积。

我们以线性回归模型为例：

$$
\ell(w_1, w_2, w_3, b) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} (x^{(i)}_1 w_1 + x^{(i)}_2 w_2 + x^{(i)}_3 w_3 + b^{(i)} - y^{(i)})^2
$$

在上式中，$\ell$ 是之前我们介绍过的平方损失函数，$n$ 是训练集样本数，$x^{(i)}_j$ 和 $y^{(i)}$ 分别是第 $i$ 个样本的特征和标签，$w_j$ 和 $b$ 分别是模型的权重和偏差参数。

我们引入 L2 范数惩罚项如下：

$$
\frac{\lambda}{2n} ||w||^2 = \frac{\lambda}{2n} (w_1^2 + w_2^2 + w_3^2)
$$

其中 $\lambda$ 是正则化**超参数**。

此时，带有 L2 范数惩罚项的新损失函数为：

$$
\ell(w_1, w_2, w_3, b) + \frac{\lambda}{2n} ||w||^2
$$

此时，根据小批量随机梯度下降法（mini-batch stochastic gradient descent），我们可以得到如下迭代公式：

$$
w_1 \leftarrow \left(1 - \eta \frac{\lambda}{|B|}\right) w_1 - \frac{\eta}{|B|} \sum_{i \in B} x_1^{(i)} \left( x_1^{(i)} w_1 + x_2^{(i)} w_2 + x_3^{(i)} w_3 + b - y^{(i)} \right)
$$

$$
w_2 \leftarrow \left(1 - \eta \frac{\lambda}{|B|}\right) w_2 - \frac{\eta}{|B|} \sum_{i \in B} x_2^{(i)} \left( x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)} \right)
$$

其中：
- $\eta$ 是学习率；
- $\lambda$ 是 L2 范数惩罚项的系数；
- $|B|$ 是小批量的大小；
- $x_1^{(i)}$ 和 $x_2^{(i)}$ 是第 $i$ 个样本的特征；
- $y^{(i)}$ 是第 $i$ 个样本的标签；
- $b$ 是偏差参数。

---

## 丢弃法

另一种应对过拟合的方法是丢弃法（dropout）。

在多层感知机中，丢弃法是指在每次迭代中随机丢弃一部分隐藏层的神经元。

$$
H = \phi(W \cdot X + b)
$$

也即是：

$$
h_i = \phi(\sum_{k=1}^{m} w_{ik} x_k + b_k)
$$

在上式中，$H$ 是隐藏层的输出，$W$ 是权重参数，$X$ 是输入数据，$b$ 是偏差参数，$\phi$ 是激活函数。

当对该隐藏层使⽤丢弃法时，该层的隐藏单元将有⼀定概率被丢弃掉。设丢弃概率为 $p$ ，那么有 $p$ 的概率 $h_i$ 会被清零，有 $1-p$ 的概率 $h_i$ 会除以 $1-p$ 做拉伸。

丢弃法的数学表达如下：

$$
h_i' = \frac{\xi_i}{1-p} h_i
$$

其中随机变量 $\xi_i$ 为 0 和 1 的概率为 $p$ 和 $1-p$ ， $h_i'$ 是丢弃法后的隐藏单元输出。

由于 $E[\xi_i] = 1-p$，所以 $E[h_i'] = h_i$ 。因此，丢弃法不会改变模型的期望输出。

