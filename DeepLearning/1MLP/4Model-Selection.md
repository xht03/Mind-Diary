---
title: "模型选择"
author: "xht03"
date: 2025-03-21 14:08:10
tags:
- labs
- notes
categories:
- Deep Learning
header-includes:
- \usepackage{xeCJK}
---

## 训练误差和泛化误差

在实际应用中，你你也许发现了：当模型在训练集上更准确时，它在测试数据集上却不⼀定更准确。这是为什么呢？

在解释这个现象之前，我们先引入两个概念：

- **训练误差**：模型在训练集上的误差。

- **泛化误差**：模型在任意⼀个测试数据样本上表现出的误差的期望，并常常通过测试集上的误差来近似。

计算训练误差和泛化误差可以使⽤之前介绍过的损失函数，例如线性回归⽤到的平⽅损失函数和 softmax 回归⽤到的交叉熵损失函数。

让我们以⾼考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年⾼考试题（训练题）时的错误率，泛化误差则可以通过真正参加⾼考（测试题）时的答题错误率来近似。假设训练题和测试题都随机采样于⼀个未知的依照相同考纲的巨⼤试题库。如果让⼀名未学习中学知识的小学⽣去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成⼀名反复练习训练题的⾼三备考⽣答题，即使在训练题上做到了错误率为 0，也不代表真实的⾼考成绩会如此。

模型的参数是通过在训练集上训练模型而学习出的，参数的选择依据了最小化训练误差（如同⾼三备考⽣）。所以，训练误差的期望 $\leq$ 泛化误差。也就是说，⼀般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于在测试数据集上的表现。由于⽆法从训练误差估计泛化误差，**⼀味地降低训练误差并不意味着泛化误差⼀定会降低**。

---

## 验证数据集

在机器学习中，通常需要评估若⼲候选模型的表现，可供选择的候选模型可以是有着不同超参数的同类模型。我们需要从中选择有效的模型。这⼀过程称为**模型选择（model selection）**。

我们考虑到：

- 从严格意义上讲，测试集只能在所有超参数和模型参数选定后使⽤⼀次。

- 不可以使⽤测试数据来选择模型，如调参。

- 由于⽆法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。

鉴于此，我们可以预留⼀部分在训练集和测试集以外的数据来进⾏模型选择。这部分数据被称为验证数据集，简称**验证集（validation set）**。

然而在实际应⽤中，由于数据不容易获取，测试数据极少只使⽤⼀次就丢弃。因此，**验证集和测试集往往不做区分**。也就是说，验证集和测试集可能是同⼀组数据，只是使⽤的时机不同。

---

## K 折交叉验证

由于验证数据集不参与模型训练，当训练数据不够⽤时，预留⼤量的验证数据显得太奢侈。

一种改善的⽅法是**K 折交叉验证（K-fold cross-validation）** ：

1. 把原始训练集分割成 K 个不重合的⼦数据集。

2. 每次从 K 个⼦数据集中选取⼀个作为验证集，剩余的 K-1 个作为训练集。

3. 做 K 次模型训练和验证。每次⽤来验证模型的⼦数据集都不同。

4. 对这K次训练误差和验证误差分别求平均。