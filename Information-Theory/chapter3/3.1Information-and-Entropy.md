---
title: "Chapter 3: Entropy (1)"
date: 2025-02-26 13:44:10
author: "xht03"
tags:
- translation
- notes
categories:
- Information and Coding Theory
header-includes:
- \usepackage{xeCJK}
---


> “简洁是智慧的灵魂。” --《哈姆雷特》
>
> 本章的目的是介绍熵函数，它衡量源发出的信息量。我们将研究这个函数的基本性质，并展示它与源的编码的平均码长之间的关系。

## 3.1 Information and Entropy

为了量化源 $S$ 的符号 $s_i$ 所传递的信息，我们为每个 $i$ 定义一个数值 $I(s_i)$，它表示在知道 $S$ 发出了 $s_i$ 时我们所获得的**信息量**；这也代表了我们在知道 $s_i$ 是否会被发出之前的**先验不确定性**（我们不确定该符号是否会出现），以及符号出现后所带来的**意外程度(surprise)**。因此，我们要求满足以下条件：

1. $I(s_i)$ 是 $s_i$ 的概率 $p_i$ 的递减函数，当 $p_i = 1$ 时，$I(s_i) = 0$；

2. $I(s_is_j) = I(s_i) + I(s_j)$。

条件 (1) 断言：**事件的概率越大，它传递的信息量越少**，一个必然发生的事件传递的信息量为零；报社编辑在选择新闻时常常遵循这一原则。条件 (2) 断言，由于源 $S$ 发出的符号是独立的（正如我们所假设的），**通过了解两个连续符号所获得的信息量是这两个符号各自信息量的和**。（如果连续符号不是独立的，那么获得的信息量会少于两者之和，因为了解 $s_i$ 会告诉我们一些关于 $s_j$ 的信息。）

源 $S$ 中符号的独立性意味着：
  
$$ 
\text{Pr}(s_i, s_j) = \text{Pr}(s_i) \cdot \text{Pr}(s_j) = P_i \cdot P_j \quad \forall \ i, j. 
$$

因此，如果我们定义**信息量**为：

$$ I(s_i) = - \log p_i = \log \frac{1}{p_i}, $$

则条件 (1) 和 (2) 将得到满足，并且：

$$ I(s_is_j) = \log \frac{1}{p_i p_j} = \log \frac{1}{p_i} + \log \frac{1}{p_j} = I(s_i) + I(s_j). $$

由于：当 $P_i \to 0$，$I(s_i) \to +\infty$ ，我们约定：  
$$ I(s_i) = +\infty \quad \text{When} \ \ p_i = 0. $$

该函数的图形如图 3.1 所示。

![图3.1](https://ref.xht03.online/202502261423388.png)

选择对数的底数并不非常重要。我们通常选择对数底数为 $r$，其中 $r$ 是编码符号的数量，因此在最常见的二进制情况下，我们有：$log = lg = \log_2$ 。对数底数的变化只是单位的变化，因为：

$$ x = r^{log_r x} \ , \  \forall x > 0$$

取对数时，改变底数为 $s$ 会得到：

$$ \log_s x = log_sr \cdot log_rx $$

在二进制情况下，信息的单位称为比特（binary digits）。如果 $ r $ 不重要或已被理解，我们将写作 $I(s_i) = -\log(p_i)$ ；如果我们希望强调 $ r $ 的值，则写作 $I_r(s_i) = -\log_r(p_i)$ 。

> **例 3.1**  
> 设 $ S $ 是一个公平的硬币，$ s_1 $ 和 $ s_2 $ 分别表示正面和反面。则 $p_1 = p_2 = \frac{1}{2}$ ，所以如果我们取 $r = 2$ ，则 $I_2(s_1) = I_2(s_2) = 1$ 。因此，信息的标准单位就是从一次公平的硬币投掷中所获得的信息量。

由于源 $ S $ 的每个符号 $ s_i $ 以概率 $ p_i $ 发射，因此，源 $ S $ 传递的平均信息量（**每个源符号的平均信息量**）由以下函数给出，称为 $ r $ -进制**熵**：

$$ H_r(S) = \sum_{i=1}^{q} p_i I_r(s_i) = - \sum_{i=1}^{q} p_i \log_r p_i. $$

与函数 $I$ 类似，换底 $r$ 相当于单位的变化，具体为：

$$ H_s(S) = log_sr \cdot H_r(S) $$

当 $ r $ 被理解或不重要时，我们通常写作：

$$ H(S) = - \sum_{i=1}^{q} p_i \log p_i $$

由于当 $p \to 0$ 时 $p \log \left( \frac{1}{p} \right) = -p \log p$ 趋近于 0（见图 3.2），我们采用约定 $p \log \left( \frac{1}{p} \right) = 0$ 当 $p = 0$ ，这样使得 $H(S)$ 是概率 $p_i$ 的连续函数。

![图3.2](https://ref.xht03.online/202502261433125.png)

> **例 3.2**
> 设 $S$ 有 $q = 2$ 个符号，概率分别为 $p$ 和 $1 - p$ ；因此，$S$ 可以表示一次投掷硬币的结果，可能是有偏的。我们将经常使用这个概率分布，为了方便，我们引入符号
>
> $$ \bar{p} = 1 - p, \quad \text{where} \quad 0 \leq p \leq 1. $$
> 
> （这里的符号 $P$ 不应与复共轭操作混淆，本书中并不使用复共轭）。然后，
> 
> $$ H(S) = -p \log p - \bar{p} \log \bar{p} $$

我们也将这个重要的函数记为 $H(p) = H_r(p) = -p \log p - \bar{p} \log \bar{p}$ 。

图3.3给出了函数 $H_2(p)$ 的图形；对于一般的 $ r $ ，我们只需要将垂直尺度乘以 $\log_r2$ 的因子。由此可以看出，$ H(p) $ 在 $p = \frac{1}{2}$ 时最大（即 1），在 $p = 0$ 或 $p = 1$ 时最小（即 0）。因此，关于 $ S $ 的不确定性最大和最小时，意味着 $ S $ 传递的信息也分别最大和最小。请注意，图形关于垂直线 $p = \frac{1}{2}$ 对称，也即是 $H(p) = H(\bar{p})$ 。
 
![图3.3](https://ref.xht03.online/202502261448195.png)
 
如果我们在例3.2中，令 $p = \frac{2}{3}$ ，我们可以得到：

$$
H_2(S) = \frac{2}{3} log_2\frac{3}{2} + \frac{1}{3} log_23 = 0.918
$$
 
因此，这个偏置硬币所传递的信息少于在示例3.1中讨论的公平硬币，后者 $H_2(S) = 1$ 。

> **例 3.3**
> 如果源 $ S $ 有 $q = 5$ 个符号，其概率分别为 $p_1 = 0.3,\ p_2 = 0.2,\ p_3 = 0.2,\ p_4 = 0.2,\ p_5 = 0.1\$ ，如在 §2.2 例2.5中所示，我们可以得出 $H_2(S) \approx 2.246$ 。

我们还可以将这些源 $ S $ 的熵与通过二进制霍夫曼编码得到的平均字长进行比较。例如，在例3.2中，当 $p = \frac{2}{3}$ 时，我们发现对于 $n = 1, 2, 3$ ，通过二进制霍夫曼编码源 $S^n$ 得到的平均字长分别为 $L \approx 1, \ 0.944, \ 0.938$ ，这些值接近熵 $H_2(S) \approx 0.918$ 。在例2.5中得到的平均字长 $L(C) = 2.3$ 接近于我们在例3.3中计算的熵 $H_2(S) \approx 2.246$ 。平均字长与熵之间的紧密关系展示了**香农第一定理**，我们将在第3.6节中陈述并证明该定理。

> **例 3.6** 
> 通过使用字母表中已知字母的频率，英语文本的熵值被计算为大约 4.03。


最后这一例子似乎表明，读一本书所传递的信息大约是投掷硬币的四倍，这说明了信息论并不关心消息的有用性或趣味性，因为这些因素很大程度上依赖于接收信息的个人。因此，一个统计学家可能会很高兴收到一本包含随机数字或字母的书，而普通人则可能更喜欢一本小说，即便它的熵值较低。
