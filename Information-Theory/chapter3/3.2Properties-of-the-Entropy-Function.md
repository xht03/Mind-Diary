---
title: "Chapter 3: Entropy (2)"
date: 2025-03-20 12:16:10
author: "xht03"
tags:
- translation
- notes
categories:
- Information and Coding Theory
header-includes:
- \usepackage{xeCJK}
---

## 3.2 Properties of the Entropy Function

在 §3.1 中，我们定义了具有概率 $p_i$ 的信源 $S$ 的熵为：

$$
H_r(S) = \sum_i p_i \log_r \frac{1}{p_i}.
$$

由于 $p \log_r (1/p) \geq 0$ ，并且当且仅当 $p = 0$ 或 $p = 1$ 时等号成立，我们有：

> **定理 3.7**  
> $H_r(S) \geq 0$ ，并且，当且仅当存在某个 $i$ 使得 $p_i = 1$ （从而对于所有 $j \neq i$ 有 $p_j = 0$ ）时等号成立。  

因此，当信源 $S$ 发出的符号没有不确定性时，即某个符号总是出现，从而不传递任何信息时，熵最小。那么，熵何时最大呢？要回答这个问题，我们需要：

> **引理 3.8**
> 对于所有 $x > 0$ ，有 $\ln x \leq x - 1$，并且当且仅当 $x = 1$ 时等号成立。
>
> ![](https://ref.xht03.online/202503201223107.png)

将其转换为以某个其他底 $r$ 的对数，我们有：$\log_r x \leq (x - 1) \log_r e$ ，并且当且仅当 $x = 1$ 时等号成立。这个结果看起来有些技术性，但它具有许多非常有用的推论。

> **推论 3.9**
> 设 $x_i \geq 0$ 且 $y_i > 0$ ，其中 $i = 1, \dots, q$ ，并且满足 $\sum_{i=1}^q x_i = \sum_{i=1}^q y_i = 1$ （因此 $(x_i)$ 和 $(y_i)$ 是概率分布，且 $y_i \neq 0$ ）。则有：
>
> $$
> \sum_{i=1}^q x_i \log_r \frac{1}{x_i} \leq \sum_{i=1}^q x_i \log_r \frac{1}{y_i}
> $$
> 
> 即：
> 
> $$
> \sum_{i=1}^q x_i \log \frac{y_i}{x_i} \leq 0
> $$
>
> 当且仅当对于所有 $i$ ，$x_i = y_i$ 时，等号成立。

**证明**

如果对于所有 $i$，$x_i \neq 0$ ，则不等式左侧 (LHS) 与右侧 (RHS) 之差为：

$$
\begin{aligned}
\text{LHS} - \text{RHS} & = \sum_{i=1}^q x_i \log_r \frac{y_i}{x_i}\\
& = \sum_{i=1}^q x_i \cdot \frac{\ln (y_i / x_i)}{\ln r} \quad (\text{因为} \log_r x = \frac{\ln x}{\ln r})\\
& \leq \frac{1}{\ln r} \sum_{i=1}^q x_i \left( \frac{y_i}{x_i} - 1 \right)\\
& = \frac{1}{\ln r} \left( \sum_{i=1}^q y_i - \sum_{i=1}^q x_i \right) \quad (\text{由引理 3.8，应用 } q \text{ 次})\\
& = 0\\
\end{aligned}
$$

等号成立的条件是对于每个 $i$ ，$\frac{y_i}{x_i} = 1$，即 $y_i = x_i$ 。当某些 $x_i = 0$ 时，论证类似。因为我们约定 $X_i \log_r (1 / x_i) = 0$ （当 $x_i = 0$ 时），可以忽略这些项的影响。$\square$

> **定理 3.10**
> 如果一个信源 $S$ 有 $q$ 个符号，则 $H_r(S) \leq \log_r q$ ，当且仅当所有符号的概率相等时，等号成立。

**证明**

设 $x_i = p_i$（$S$ 的各个符号的概率），$y_i = \frac{1}{q}$，则满足推论 3.9 的条件。因此，我们有：

$$
H_r(S) = \sum_{i=1}^q p_i \log_r \frac{1}{p_i} \leq \sum_{i=1}^q p_i \log_r q = \log_r q \sum_{i=1}^q p_i = \log_r q
$$

当且仅当每个 $p_i = \frac{1}{q}$ 时，等号成立。 $\square$

因此，当信源发出的符号具有最大不确定性时，熵值最大，传递的信息量也最多。