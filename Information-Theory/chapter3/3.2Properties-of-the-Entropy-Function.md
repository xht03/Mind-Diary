---
title: "Chapter 3: Entropy (2)"
date: 2025-03-20 12:16:10
author: "xht03"
tags:
- translation
- notes
categories:
- Information and Coding Theory
header-includes:
- \usepackage{xeCJK}
---

## 3.2 Properties of the Entropy Function

在 §3.1 中，我们定义了具有概率 $p_i$ 的信源 $S$ 的熵为：

$$
H_r(S) = \sum_i p_i \log_r \frac{1}{p_i}.
$$

由于 $p \log_r (1/p) \geq 0$ ，并且当且仅当 $p = 0$ 或 $p = 1$ 时等号成立，我们有：

> **定理 3.7**  
> $H_r(S) \geq 0$ ，并且，当且仅当存在某个 $i$ 使得 $p_i = 1$ （从而对于所有 $j \neq i$ 有 $p_j = 0$ ）时等号成立。  

因此，当信源 $S$ 发出的符号没有不确定性时，即某个符号总是出现，从而不传递任何信息时，熵最小。那么，熵何时最大呢？要回答这个问题，我们需要：

> **引理 3.8**
> 对于所有 $x > 0$ ，有 $\ln x \leq x - 1$，并且当且仅当 $x = 1$ 时等号成立。
>
> ![](https://ref.xht03.online/202503201223107.png)

将其转换为以某个其他底 $r$ 的对数，我们有：$\log_r x \leq (x - 1) \log_r e$ ，并且当且仅当 $x = 1$ 时等号成立。这个结果看起来有些技术性，但它具有许多非常有用的推论。


