---
title: "Chapter 3: Entropy (3)"
date: 2025-03-24 14:00:10
author: "xht03"
tags:
- translation
- notes
categories:
- Information and Coding Theory
header-includes:
- \usepackage{xeCJK}
---

## 3.3 Entropy and Average Word Length

在 §3.1 接近尾声时，我们考虑了几个信源，并比较了它们的熵与哈夫曼编码的平均字长。我们现在将更详细地探讨熵与平均字长之间的关系。

> **定理 3.11**
> 如果 $C$ 是信源 $S$ 的一个唯一可解码的 $r$ 进制码，则 $L(C) \geq H_r(S)$ 。

**证明**
我们定义：

$$
K = \sum_{i=1}^q r^{-l_i}
$$

其中 $C$ 的字长为 $l_1, \dots, l_q$ 。根据 McMillan 不等式（定理 1.21），有 $K \leq 1$ 。现在应用推论 3.9，定义 $x_i = p_i$ （信源 $S$ 的符号概率），$y_i = \frac{r^{-l_i}}{K}$ ，满足 $y_i > 0 $ 且 $\sum_{i=1}^q y_i = 1$ 。则：

$$
\begin{aligned}
H_r(S) & = \sum_{i=1}^q p_i \log_r \frac{1}{p_i}\\
& \leq \sum_{i=1}^q p_i \log_r \frac{1}{y_i} \quad \text{根据推论 3.9} \\
& = \sum_{i=1}^q p_i \log_r \left( {r^{l_i}K} \right)\\
& = \sum_{i=1}^q p_i (l_i + \log_r K)\\
& = \sum_{i=1}^q p_i l_i + \log_r K \sum_{i=1}^q p_i\\
& = L(C) + \log_r K \quad (\text{因为} \sum_{i=1}^q P_i = 1)\\
& \leq L(C) \quad (\text{因为} K \leq 1)
\end{aligned}
$$

因此，$L(C) \geq H_r(S)$ 。$\square$

这一定理可以如下理解：

信源 $S$ 发出的每个符号平均携带 $H_r(S)$ 个单位的信息。为了在编码过程中不丢失任何信息，编码 $C$ 必须是唯一可解码的。每个编码符号传递一个单位的信息，因此 $C$ 的每个编码词平均必须至少包含 $H_r(S)$ 个编码符号，即 $L(C) \geq H_r(S)$ 。特别地，传递更多信息的信源需要更长的编码词。

> **推论 3.12**
> 给定一个具有概率 $p_i$ 的信源 $S$ ，存在一个唯一可解码的 $r$ 进制码 $C$ 使得 $L(C) = H_r(S)$ ，当且仅当对于每个 $i$ ，$\log_r p_i$ 是一个整数，即每个 $p_i = r^{e_i}$ ，其中 $e_i \leq 0$ 是一个整数。

**证明**
($\Rightarrow$)
如果在定理 3.11 的证明中 $L(C) = H_r(S)$ ，那么其中的两个不等号必须同时取等号。根据推论 3.9，这意味着对于每个 $i$ ，$p_i = y_i$ ，并且 $\log_r K = 0$ 。由此得出 $K = 1$ ，且：$p_i = \frac{r^{-l_i}}{K} = r^{-l_i}$ ，因此：$\log_r p_i = -l_i$ ，这是一个整数。

**($\Leftarrow$)**
假设对于每个 $i$ ，$-\log_r p_i$ 是一个整数 $l_i$ 。由于 $p_i \leq 1$ ，我们有 $l_i \geq 0$ 。现在：$r^{l_i} = \frac{1}{p_i}$ 。所以：

$$
\sum_{i=1}^q \frac{1}{r^{l_i}} = \sum_{i=1}^q p_i = 1
$$

这满足 McMillan 不等式（定理 1.21），因此存在一个唯一可解码的 $r$ 进制码 $C$ ，其字长为 $l_i$ 。此码的平均字长为：

$$
L(C) = \sum_{i=1}^q p_i l_i = H_r(S)
$$

$\square$

推论 3.12 中条件 $p_i = r^{e_i}$ 是**非常苛刻**的。对于大多数信源，每一个唯一可解码的编码都满足 $L(C) > H_r(S)$ 。

> **例 3.13**
> 如果信源 $S$ 有 $q = 3$ 个符号 $s_i$ ，其概率分别为 $p_i = \frac{1}{4}, \frac{1}{2}, \frac{1}{4}$ （如例 1.2 和 2.1），则 $S$ 的二进制熵为：
>
> $$
> H_2(S) = \frac{1}{4} \log_2{4} + \frac{1}{2} \log_2{2} + \frac{1}{4} \log_2{4} = \frac{1}{4} \cdot 2 + \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 2 = \frac{3}{2}
> $$
> 
> 考虑二进制哈夫曼编码 $C$ ：$s_1 \mapsto 00$ ，$s_2 \mapsto 1$ ，$s_3 \mapsto 01$ 。这是 $S$ 的一个最优编码，其平均字长为：
> 
> $$
> L(C) = \frac{1}{4} \cdot 2 + \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 2 = \frac{3}{2}
> $$
> 
> 因此，在此情况下，对于某个唯一可解码的二进制码 $C$ ，$L(C) = H_2(S)$ 。这是因为所有的概率 $p_i$ 均为 2 的幂次。

> **例 3.14**
>
> 设信源 $S$ 有 $q = 5$ 个符号，其概率分别为 $p_i = 0.3, 0.2, 0.2, 0.2, 0.1$ （参见 §2.2，例 2.5）。我们在示例 3.3 中看到 $H_2(S) \approx 2.246$ ，在例 2.5 中看到 $S$ 的二进制哈夫曼编码的平均字长为 2.3。因此，根据定理 2.8，对于 $S$ 的每一个唯一可解码的二进制码 $C$ ，有：
>
> $$
> L(C) \geq 2.3 > H_2(S)
> $$
> 
> 这表明不存在满足 $L(C) = H_2(S)$ 的唯一可解码二进制码。原因在于，此例中概率 $p_i$ 并非全都为 2 的幂次。

根据推论 3.12，如果某些 $p_i = 0$ ，则必然有 $L(C) > H_r(S)$ 。然而，通过删除这些符号 $s_i$ ，我们可能实现等号成立，即通过允许使用更短的编码词来减少 $L(C)$ ，而熵 $H_r(S)$ 保持不变。

> **例 3.15**
> 设信源 $S$ 有三个符号 $s_i$，其概率分别为 $p_i = \frac{1}{2}, \frac{1}{2}, 0$ 。则：$H_2(S) = 1$ ，但 $S$ 的二进制哈夫曼编码 $C$ 的字长为 1, 2, 2，其平均字长为：$L(C) = 1.5$ 。然而，如果去掉概率为 0 的符号 $s_3$ ，则 $H_2(S) = 1$ ，此时可以使用编码 $C = \{0, 1\}$ ，其平均字长为：$L(C) = 1$ ，从而 $H_2(S) = 1 = L(C)$ 。在这里可以实现等号，是因为剩余的非零概率 $p_i$ 均为 $r = 2$ 的幂次。

如果 $C$ 是信源 $S$ 的一个 $r$ 进制码，我们定义其**效率**为：

$$
\eta = \frac{H_r(S)}{L(C)} \tag{3.4}
$$

根据定理 3.11，对于每个唯一可解码的编码 $C$，有 $0 \leq \eta \leq 1$。编码 $C$ 的**冗余**定义为：$\bar{\eta} = 1 - \eta$ 。因此，冗余的增加会降低效率。在示例 3.13 和 3.14 中，效率分别为 $\eta = 1$ 和 $\eta \approx 0.977$ 。

