---
title: "Chapter 2: Optimal Codes (6)"
author: "xht03"
date: 2025-03-18 09:42:30
tags:
- translation
- notes
categories:
- Information and Coding Theory
header-includes:
- \usepackage{xeCJK}
---

## 2.6 Extensions of Sources

与其一次编码一个源符号 $s_i$ ，更高效的方法是对连续的符号块进行编码。例如，在文本中，与其单独编码每个字母，不如对整个单词（甚至句子）进行编码。这种方法能提供更丰富的概率变化，从而降低平均码长（正如§2.2中所述）。

设 $S$ 为一个源，其源字母表为 $S$ ，包含 $q$ 个符号 $s_1, \dots, s_q$ ，其对应的概率为 $p_1, \dots, p_q$ 。源 $S$ 的第 $n$ 次扩展 $S^n $是一个新的源，其源字母表 $S^n$ 由 $q^n$ 个符号 $s_{i_1} \dots s_{i_n}$ 组成（其中 $s_{i_j} \in S^n$ ），每个符号的概率为：$p_{i_1} \cdots p_{i_n}$ 。我们可以将 $s_{i_1} \dots s_{i_n}$ 看作是来自 $S$ 的 $n$ 个连续符号组成的一个块，或者等价地视为 $n$ 个独立的 $S$ 副本同时输出一个符号（例如，想象同时投掷多个相同的硬币或掷多个相同的骰子）。我们可以验证，概率 $p_{i_1} \cdots p_{i_n}$ 形成一个概率分布，只需展开以下等式的左侧：

$$ (p_1 + \cdots + p_q)^n = 1^n = 1 $$

并注意到每个$p_{i_1} \cdots p_{i_n}$在展开中恰好出现一次。

> **例 2.11**
> 设$S$的源字母表为 $S = {s_1, s_2}$ ，其中 $p_1 = \frac{2}{3}$，$p_2 = \frac{1}{3}$ 。那么，$S^2$ 的源字母表为：$ S^n = \{s_1s_1, s_1s_2, s_2s_1, s_2s_2\}$ ，其对应的概率为：$\frac{4}{9}, \frac{2}{9}, \frac{2}{9}, \frac{1}{9}$ 。

一般来说，设 $p_1$ 和 $p_q$ 分别是 $S$ 中最大的和最小的概率，那么 $S^n$ 中的最大和最小概率也分别是 ${p_1}^n$ 和 ${p_q}^n$ 。

假设$p_1 > p_q$（即，概率 $p_i$ 不全相等于 $1/q$ ），那么我们有：

$$\frac{p_1^n}{p_q^n} \to \infty \quad \text{When} \quad n \to \infty$$

这意味着，随着 $n$ 的增加，$S^n$ 的概率分布变得更加不均匀，因此可以期望更高效的编码。

> **例 2.12**
> 如果 $S$ 如例 2.11 所示，则存在一个二进制 Huffman 编码 $C$ ：$s_1 \to 0, \quad s_2 \to 1$ ，其平均码长为：$L(C) = 1$ ，乍一看，似乎无法进一步优化，但我们仍然可以为 $S^2$ 构造 Huffman 编码。我们使用 §2.2 描述的算法进行如下操作（每行的概率未按降序排列）：
>
> |            |                |                |                  |               |            |           |             |           |
> |------------|----------------|----------------|------------------|---------------|------------|-----------|-------------|-----------|
> | $S^2$      | $\frac{4}{9}$  | $\frac{2}{9}$  | $\frac{2}{9}$    | $\frac{1}{9}$ |      0     |     10    |     110     |    111    | 
> | $(S^2)'$   | $\frac{4}{9}$  | $\frac{2}{9}$  | $\frac{3}{9}$    |               |      0     |     10    |     11      |           |
> | $(S^2)''$  | $\frac{4}{9}$  | $\frac{5}{9}$  |                  |               |      0     |     1     |             |           |
> | $(S^2)'''$ | $1$            |                |                  |               | $\epsilon$ |           |             |           |
>
> 这给出了 $S^2$ 的 Huffman 编码 $C^2$ ：$ s_1 s_1 \to 0, \quad s_1 s_2 \to 10, \quad s_2 s_1 \to 110, \quad s_2 s_2 \to 111 $ ，其平均码长为：$L_2 = L(C_2) = \frac{2}{9} + \frac{3}{9} + \frac{5}{9} + \frac{1}{9} = \frac{17}{9}$ 。
>
> 由于 $C^2$ 中的每个码字代表 $S$ 的两个符号块，因此平均而言，每个 $S$ 的符号需要：$\frac{L_2}{2} = \frac{17}{18} \approx 0.944$ 。这小于 $S$ 的 Huffman 编码 $C$ 的平均码长 $L(C) = 1$ ，因此这种编码方式更高效。

严格来说，我们所描述的并不是 $S$ 的一个编码，因为 $S$ 的各个单独符号并未被分配它们自己的编码词；尽管如此，它使我们能够对来自 $S$ 的信息进行编码，因此我们称之为 $S$ 的编码。这样的编码是唯一可解码的：作为 $S^2$ 的编码，Huffman 码 $C^2$ 是瞬时的，因此是唯一可解码的；这意味着我们可以将任何编码序列 $t$ 唯一地分解为编码词，从而确定编码在 $t$ 中的 $S^2$ 的符号 $s_{i_1}s_{i_2}$ ，并由此得到编码在 $t$ 中的 $S$ 的各个单独符号 $s_i$ 。然而，这种解码并非完全瞬时的：我们必须成对地依次确定 $S$ 的符号，而不是一次确定一个，因此在等待配对完成时**存在一定的有限延迟**。

继续这一原则，可以证明 $S^3$ 的 Huffman 码 $C^3$ 的平均词长 $L_3 = L(C^3) = 76/27$ ；作为 $S$ 的编码，它的平均词长为
$$
\frac{L_3}{3} = \frac{76}{81} = 0.938\ldots
$$

这比使用 $C_2$ 还要好。

这一想法显然可以扩展到任意 $n$ 的 $S^n$ ，由此引出了两个自然的问题：当 $n \to \infty$ 时，平均词长 $L_n / n$ 会发生什么变化，其中 $L_n = L(C_n)$ ；以及我们能否应用相同的方法来获得其他信源更有效的编码？要回答这些问题，我们需要引入下一个重要主题，即**熵（entropy）**。



